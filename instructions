--Instructions--
#open a ipython terminal at the pfiles directory
#import the skyglowdb module:
import skyglowdb
#The skyglow IMG files (or folders) must be put into a folder 
#named "glow" under the main HD directory.
#The cloud data works the same way, it must be under the "cloud" folder.
#load the database using the root drive path.
db = skyglowdb.DB("/media/lava-h")
#Start the database indexing process:
db.build()
#This will take a while. (perhaps around 4 hours).
#Once the building is complete, it is recommended that you 
#backup the database which simply stores a copy.
db.backup()
#At this point you are free to produce various data products. 
#Their methods generally start with "Make"
#   Example: db.makeCollages()
#For your convenience, i packed all of the currently available methods into a single process:
db.makeDataPoducts()
#This should begin the long process of building all of the various data products...

--Chaining--
#I've made it possible to "chain" most of the methods:
db.build().makeDataProducts()
#This command will run build, then makeDataProducts, which should accomplish the task of 
#processing an entire drive.

--General Notes--
I wrote 57 regular images in 5.0406 seconds, the same images using nostar filter took 22.83599 minutes
22.83599*60/5.0406 = 271.82466373050823
In my testing nostar takes between 240 to 270 times longer than regular.
If this is right, it is extremely significant. 
It is worth asking whether the client even cares about this.
At any rate, this will take quite a long time.

Images for Video appears to take about 5 hours (building every frame).


I've made it so that the getImg function knows where the files should be saved, 
this turned out to be a fiasco, but this allows the getImg function to return the ready made file,
instead of building a new one. This is important if we want to make nostar collages.

I have not been able to make any sense of the cloud data, the images appear to be rather useless for detecting clouds, and the data doesn't match up